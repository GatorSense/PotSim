{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d607d44-b7a6-4302-b73c-e9cfcaf6b1d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:21:56.290663Z",
     "iopub.status.busy": "2025-01-31T22:21:56.290540Z",
     "iopub.status.idle": "2025-01-31T22:22:27.694105Z",
     "shell.execute_reply": "2025-01-31T22:22:27.693609Z",
     "shell.execute_reply.started": "2025-01-31T22:21:56.290647Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# System and other standard libraries\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Data processing, metrics and plot libraries\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "# Pytorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.multiprocessing as mp\n",
    "from torchinfo import summary\n",
    "\n",
    "# Custom packages \n",
    "from utils import preprocessing as ppsr\n",
    "from utils.potsimloader import potsimloader as psl\n",
    "from utils import split\n",
    "from models import linearregression, mlp, cnn, tcn, lstm, transformer\n",
    "from utils import scaler\n",
    "from training import train\n",
    "from testing import evaluate\n",
    "\n",
    "# Enabling polars global string cache, for more informaiton read below:\n",
    "# https://docs.pola.rs/api/python/stable/reference/api/polars.StringCache.html\n",
    "pl.enable_string_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06214f0e-d709-4086-9360-7d84bc8c401b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:22:27.706446Z",
     "iopub.status.busy": "2025-01-31T22:22:27.706328Z",
     "iopub.status.idle": "2025-01-31T22:22:27.717604Z",
     "shell.execute_reply": "2025-01-31T22:22:27.717215Z",
     "shell.execute_reply.started": "2025-01-31T22:22:27.706433Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Set seed for Python's random module\n",
    "    np.random.seed(seed)  # Set seed for NumPy\n",
    "    \n",
    "    # Setting seed for PyTorch devices\n",
    "    torch.manual_seed(seed)  # For CPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)  # For current GPU (if using CUDA)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43322abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting device type to be trained as \"cuda\" if aviablable else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates Nitrogen(N) treatment as Npre/Npl-Nemg-Ntu \n",
    "# using the fertilizer values from 'n_values'\n",
    "# such that (Npre/Npl + Nemg + Ntu) <= 400 kg/ha \n",
    "def generate_treatments(n_values):\n",
    "    combis = list(itertools.product(n_values, repeat=3))\n",
    "    return [\"-\".join(map(str, t)) for t in combis if sum(t) <= 400]\n",
    "\n",
    "\n",
    "# Used to combine the yearly data from Dataverse into a \n",
    "# single complete file with '.parquet' extension named \n",
    "# 'potsim.parquet', saved to 'data' folder of the main directory\n",
    "def join_potsim_yearly(data_dir, save_dir=Path(\"data\"), save=True):\n",
    "    if not save:\n",
    "        return\n",
    "    data_dir = Path(data_dir).resolve()\n",
    "    files = os.listdir(data_dir)\n",
    "    pattern = re.compile(r\"^potsim_\\d{4}\\.parquet$\")\n",
    "    files = sorted([file for file in files if pattern.match(file)])\n",
    "    files = [data_dir / file for file in files]\n",
    "    df = pl.scan_parquet(files)\n",
    "    filepath = save_dir / \"potsim.parquet\"\n",
    "    df.sink_parquet(\n",
    "        filepath,\n",
    "        statistics=True,\n",
    "        compression=\"zstd\",\n",
    "        compression_level=1,\n",
    "        row_group_size=1_000_000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14752e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that we would require to run the experiments.\n",
    "# Modify in case of running custom experiments.\n",
    "usecols = ['Year', 'Date', 'Treatment', 'NFirstApp','PlantingDay', 'IrrgDep',\n",
    "           'IrrgThresh', 'DayAfterPlant', 'NApp', 'NLeach', 'NPlantUp', 'NTotL1', \n",
    "           'NTotL2', 'Irrg', 'SWatL1', 'SWatL2', 'Rain', 'SolarRad', 'AirTempC']\n",
    "   \n",
    "# For our training we only need data from one day before the first \n",
    "# application of fertilizer i.e. `Npre` or `Npl`. So, we create a \n",
    "# `mask` to filter the data later.\n",
    "mask = (\n",
    "    ((pl.col(\"NFirstApp\") == \"Npl\") & (pl.col(\"DayAfterPlant\") >= -1)) |\n",
    "    ((pl.col(\"NFirstApp\") == \"Npre\") & (pl.col(\"DayAfterPlant\") >= -37))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining filepaths for all the data files\n",
    "potsim_yearly_dir = Path(\"data/potsim_yearly\")\n",
    "weather_file = Path(\"data\") / \"weather.parquet\"\n",
    "data_file = Path(\"data\") / \"potsim.parquet\"\n",
    "\n",
    "# Change save=False if yearly potsim data already combined\n",
    "join_potsim_yearly(potsim_yearly_dir, save_dir=Path(\"data\"), save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d15b5",
   "metadata": {},
   "source": [
    "#### Defining the variables for the experiments\n",
    "Modify according to your the requirments of the experiments\n",
    "- `n_values`: The amount of N fertilizer to apply\n",
    "- `scenario_filter`: The crop management scneraios we require to fetch from the actual dataset\n",
    "- `(train/val/test)_years`: The respective years of the train/val/test split for experiments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ee255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible values for n_values can be [0, 28, 56, 84, 112, 140, 168, 196]\n",
    "n_values = [0, 56, 112, 168, 196]\n",
    "treatments = generate_treatments(n_values)\n",
    "\n",
    "# Please refer to the paper for all the possible values of each scenarios variable\n",
    "scenario_filter= {\n",
    "    \"Year\": list(range(2001, 2025)),\n",
    "    \"Treatment\": treatments,\n",
    "    \"PlantingDay\": [1, 15, 29, 43, 57],\n",
    "    \"IrrgDep\": [30, 40, 50, 60],\n",
    "    \"IrrgThresh\": [50, 60, 70, 80, 90],\n",
    "    \"NFirstApp\": [\"Npl\", \"Npre\"]\n",
    "}\n",
    "\n",
    "# Divided as an exmaple, and can be adjusted as needed\n",
    "train_years = list(range(2001, 2017))\n",
    "val_years = list(range(2017, 2021))\n",
    "test_years = list(range(2021, 2025))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fa8c40",
   "metadata": {},
   "source": [
    "For complete API documentation on `potsimloader` i.e. `psl` module, please refer to: [https://github.com/GatorSense/PotSim/tree/main/utils/potsimloader](https://github.com/GatorSense/PotSim/tree/main/utils/potsimloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf679bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the combined `potsim.parquet` data and joins it with weather \n",
    "# data (if provided), then return a polars LazyFrame. \n",
    "data = psl.read_data(\n",
    "    dataset_path=data_file,\n",
    "    weather_path=weather_file,\n",
    "    usecols=usecols,\n",
    "    lazy=True,\n",
    "    as_pandas=False,\n",
    ")\n",
    "\n",
    "# Uses the 'potsimloader` module to apply the scenario_filter`, selecting \n",
    "# the required scenarios. Returning a polars DataFrame.\n",
    "data = psl.apply_filter(data, filters=scenario_filter, lazy=False, as_pandas=False)\n",
    "\n",
    "# Applies the `mask` defined previously to further filter data for\n",
    "# experiments. Then, converts and returns a pandas DataFrame\n",
    "df = data.filter(mask).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60b1aa",
   "metadata": {},
   "source": [
    "#### Data Splitting Strategy\n",
    "\n",
    "This section implements a **two-stage data splitting approach** that ensures both temporal and scenario-based separation between training, validation, and test sets. The splitting strategy prevents data leakage by maintaining distinct crop management practices across splits while respecting temporal boundaries. The `random_sample_train_val_test` function from the `utils/split` module  first groups the data by unique combinations of five management variables, then randomly assigns these groups to training, validation, and test sets based on set ratios *(60:20:20)*, making sure each split has different management scenarios regardless of year. After assigning scenarios, each split is filtered to include only data from the relevant years. This approach prevents overlap in management practices between splits and maintains clear separation by time, ensuring a fair evaluation of model performance.\n",
    "\n",
    "\n",
    "\n",
    "<!--\n",
    "**Stage 1: Scenario-Based Splitting**\n",
    "The function first identifies unique combinations of crop management practices using five key variables: `Treatment`, `PlantingDay`, `IrrgDep`, `IrrgThresh`, and `NFirstApp`. These combinations are randomly shuffled and split according to the specified ratios (0.6:0.2:0.2), ensuring that *different management scenarios* are allocated to different splits regardless of year. For instance, if 100 unique combinations exist, 60 would be assigned to training, 20 to validation, and 20 to testing.\n",
    "\n",
    "**Stage 2: Temporal Filtering**\n",
    "After scenario assignment, the function applies temporal constraints by filtering each split (using `train_years`, `val_years`, and `test_years`) to include only data from the corresponding year ranges.\n",
    "\n",
    "This two-stage approach ensures that models cannot exploit similarities in management practices between splits while maintaining temporal separation for realistic evaluation.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fcc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data using stratified temporal approach\n",
    "# - split: tuple of ratios (train, val, test) that must sum to 1.0\n",
    "# - train_years/val_years/test_years: lists defining temporal boundaries for each split\n",
    "# - seed: integer for reproducible random sampling of crop management combinations\n",
    "train_split, val_split, test_split = split.random_sample_train_val_test(\n",
    "    df,\n",
    "    split=(0.6, 0.2, 0.2),\n",
    "    train_years=train_years,\n",
    "    val_years=val_years,\n",
    "    test_years=test_years,\n",
    "    seed = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29aad09",
   "metadata": {},
   "source": [
    "#### Model Configuration and Feature Selection\n",
    "This section configures the target variable and associated features for model training using a centralized configuration approach.\n",
    "\n",
    "**Configuration Management:**\n",
    "- All experimental configurations are defined in `utils/config.json`\n",
    "- Change `target_col` to experiment with different prediction targets\n",
    "- For custom experiments, modify `feature_cols` directly or update the configuration file at `utils/config.json`\n",
    "- Override `feature_cols` with your own feature list for exploratory analysis\n",
    "- Extend `utils/config.json` with new target variables and their corresponding feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b0efd-91be-4bf8-85c9-21ca9c17b8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:23:19.641264Z",
     "iopub.status.busy": "2025-01-31T22:23:19.640990Z",
     "iopub.status.idle": "2025-01-31T22:23:19.918391Z",
     "shell.execute_reply": "2025-01-31T22:23:19.917914Z",
     "shell.execute_reply.started": "2025-01-31T22:23:19.641247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select target variable for training/prediction\n",
    "target_col = \"NTotL1\"\n",
    "\n",
    "# Load experiment configuration from config file\n",
    "# All target variables and their associated features are predefined in config.json\n",
    "with open(Path(\"utils/config.json\"), 'r') as file:\n",
    "    metadata = json.load(file).get(target_col)\n",
    "feature_cols = metadata[\"features\"]\n",
    "\n",
    "# Create output directory for model artifacts and results\n",
    "output_dir = Path() / \"outputs\" / f\"{target_col}\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define scaler save path for reproducible preprocessing\n",
    "scaler_save_path =  output_dir / f\"{target_col}_scaler.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e6bf3",
   "metadata": {},
   "source": [
    "#### Data Processing and Feature Engineering\n",
    "This section processes the split datasets into model-ready tensors with appropriate scaling and sequence formatting for different model architectures.\n",
    "\n",
    "**Key Processing Steps:**\n",
    "- Missing values in numerical columns are filled with zeros\n",
    "- Min-Max Normalization applied to prevent feature dominance\n",
    "- For temporal models, creates sliding windows of length `seq_len`\n",
    "- Outputs PyTorch tensors ready for model training\n",
    "\n",
    "**Important Notes:**\n",
    "- Always use `mode=\"fit\"` only on training data to prevent data leakage\n",
    "- Use `mode=\"transform\"` for validation and test sets to apply the same scaling\n",
    "- Adjust `seq_len` based on your temporal model requirements and data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b771ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence length configuration:\n",
    "# - Set to None for non-sequential models (LinearRegression, MLP, etc.)\n",
    "# - Set to integer for sequential models (CNN, TCN, LSTM, etc.)\n",
    "# seq_len = None  # For non-sequential models\n",
    "seq_len = 15\n",
    "\n",
    "# Process training data: fit scaler and transform features/targets\n",
    "# Mode \"fit\" calculates scaling parameters from training data\n",
    "X_train, y_train = ppsr.process_data(\n",
    "    train_split,\n",
    "    feats=feature_cols,\n",
    "    tgt=target_col,\n",
    "    scaler_path=scaler_save_path,\n",
    "    mode=\"fit\",\n",
    "    seq_len=seq_len,\n",
    ")\n",
    "\n",
    "# Process validation data: apply existing scaler without refitting\n",
    "# Mode \"transform\" uses previously fitted scaling parameters\n",
    "X_val, y_val = ppsr.process_data(\n",
    "    val_split,\n",
    "    feats=feature_cols,\n",
    "    tgt=target_col,\n",
    "    scaler_path=scaler_save_path,\n",
    "    mode=\"transform\",\n",
    "    seq_len=seq_len,\n",
    ")\n",
    "\n",
    "# Process test data: apply existing scaler without refitting\n",
    "X_test, y_test = ppsr.process_data(\n",
    "    test_split,\n",
    "    feats=feature_cols,\n",
    "    tgt=target_col,\n",
    "    scaler_path=scaler_save_path,\n",
    "    mode=\"transform\",\n",
    "    seq_len=seq_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bae19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints the shapes of each split\n",
    "print(f\"Train shapes: \\nX_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Val shapes: \\nX_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"Test shapes: \\nX_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb0693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = metadata['models'][\"Linear Regression\"].get(\"class_name\")\n",
    "# model_params = metadata['models'][\"Linear Regression\"].get(\"params\")\n",
    "# model = linearregression.LinearRegression(**model_params)\n",
    "# model_state = output_dir / f\"{target_col}_{model_name}.pth\"\n",
    "\n",
    "# model_name = metadata['models'][\"Multi-Layered Perceptron\"].get(\"class_name\")\n",
    "# model_params = metadata['models'][\"Multi-Layered Perceptron\"].get(\"params\")\n",
    "# model = mlp.MLP(**model_params)\n",
    "# model_state = output_dir / f\"{target_col}_{model_name}.pth\"\n",
    "\n",
    "# model_name = metadata['models'][\"1-D Convolutional Neural Network\"].get(\"class_name\")\n",
    "# model_params = metadata['models'][\"1-D Convolutional Neural Network\"].get(\"params\")\n",
    "# model = cnn.CNN1D(**model_params)\n",
    "# model_state = output_dir / f\"{target_col}_{model_name}.pth\"\n",
    "\n",
    "# model_name = metadata['models'][\"Temporal Convolutional Network\"].get(\"class_name\")\n",
    "# model_params = metadata['models'][\"Temporal Convolutional Network\"].get(\"params\")\n",
    "# model = tcn.TCN(**model_params)\n",
    "# model_state = output_dir / f\"{target_col}_{model_name}.pth\"\n",
    "\n",
    "model_name = metadata['models'][\"Long Short-Term Memory\"].get(\"class_name\")\n",
    "model_params = metadata['models'][\"Long Short-Term Memory\"].get(\"params\")\n",
    "model = lstm.LSTM(**model_params)\n",
    "model_state = output_dir / f\"{target_col}_{model_name}.pth\"\n",
    "\n",
    "# model_name = metadata['models'][\"Transformer\"].get(\"class_name\")\n",
    "# model_params = metadata['models'][\"Transformer\"].get(\"params\")\n",
    "# model = transformer.EncoderOnlyTransformer(**model_params, max_seq_len=seq_len)\n",
    "# model_state = output_dir / f\"{target_col}_{model_name}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting model to device \"cuda\" if GPU available else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c50709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for calculating and printing metrics during training\n",
    "min_tgt, max_tgt = scaler.get_min_max(tgt=target_col, scaler_path=scaler_save_path)\n",
    "min_tgt = torch.tensor(min_tgt, device=device)\n",
    "max_tgt = torch.tensor(max_tgt, device=device)\n",
    "print(min_tgt, max_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe6b4a",
   "metadata": {},
   "source": [
    "#### Training Configuration and Hyperparameters\n",
    "This section defines the training hyperparameters and optimization strategy for model training with early stopping and learning rate scheduling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c59af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core training hyperparameters\n",
    "lr = 0.0001              # Learning rate - adjust based on model convergence\n",
    "b_sz = 256               # Batch size - increase for larger datasets/GPU memory\n",
    "max_epochs = 100          # Maximum training epochs\n",
    "\n",
    "# Loss function and optimization\n",
    "criterion = nn.MSELoss() \n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduling for adaptive training\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5, min_lr=1e-8\n",
    ")\n",
    "\n",
    "# Early stopping configuration\n",
    "epochs_no_improve = 0\n",
    "min_loss_reduction=1e-4\n",
    "early_stop_patience: int = 10\n",
    "\n",
    "# Mixed precision training for faster computation\n",
    "ampscaler = torch.amp.GradScaler(device=device.type)\n",
    "\n",
    "# Model and logging save paths\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_save_path = output_dir / f\"{target_col}_{model_name}.pth\"\n",
    "logs_save_path = output_dir / f\"logs_{target_col}_{model_name}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97893392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Training DataLoader with data shuffling and performance optimization\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train, y_train),\n",
    "    num_workers=10,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    batch_size=b_sz,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Validation DataLoader without shuffling for consistent evaluation\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(X_val, y_val),\n",
    "    num_workers=4,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    batch_size=b_sz,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888663b3",
   "metadata": {},
   "source": [
    "#### Model Training Loop\n",
    "This section implements the main training loop with early stopping, learning rate scheduling, and comprehensive logging for model optimization. The below cell is commented,please uncomment if you are training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa398a8-d1fb-49f2-86a2-0b0b3f6cc576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T08:07:11.944087Z",
     "iopub.status.busy": "2025-01-31T08:07:11.943766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(f\"Training {model_name}on device: {device}\")\n",
    "\n",
    "# # Initialize training state variables\n",
    "# best_val_loss = float(\"inf\")\n",
    "# train_val_logs = []\n",
    "# early_stop = False\n",
    "\n",
    "# for epoch in range(max_epochs):\n",
    "#     # Check for early stopping condition\n",
    "#     if early_stop:\n",
    "#         print(f\"Early stopping at epoch {epoch}\")\n",
    "#         break\n",
    "\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Training phase with mixed precision scaling\n",
    "#     train_loss, train_r2 = train.train_epoch(\n",
    "#         model,\n",
    "#         train_loader,\n",
    "#         optimizer,\n",
    "#         criterion,\n",
    "#         device,\n",
    "#         min_tgt,\n",
    "#         max_tgt,\n",
    "#         ampscaler=ampscaler,\n",
    "#     )\n",
    "\n",
    "#     # Validation phase without gradient computation\n",
    "#     val_loss, val_r2 = train.val_epoch(\n",
    "#         model, val_loader, criterion, device, min_tgt, max_tgt\n",
    "#     )\n",
    "\n",
    "#     # Update learning rate based on validation performance\n",
    "#     scheduler.step(val_loss)\n",
    "\n",
    "#     # Model checkpointing and early stopping logic\n",
    "#     if val_loss < best_val_loss - min_loss_reduction:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), model_save_path)\n",
    "#         epochs_no_improve = 0\n",
    "#     else:\n",
    "#         epochs_no_improve += 1\n",
    "#         if epochs_no_improve > early_stop_patience:\n",
    "#             early_stop = True\n",
    "\n",
    "#     # Log training metrics and timing\n",
    "#     elapsed = time.time() - start_time\n",
    "#     curr_lr = optimizer.param_groups[0][\"lr\"]\n",
    "#     train_val_logs.append(\n",
    "#         {\n",
    "#             \"epoch\": epoch + 1,\n",
    "#             \"train_loss\": train_loss,\n",
    "#             \"val_loss\": val_loss,\n",
    "#             \"train_r2\": train_r2,\n",
    "#             \"val_r2\": val_r2,\n",
    "#             \"learning_rate\": curr_lr,\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # Display epoch progress\n",
    "#     print(\n",
    "#         f\"Epoch: {epoch + 1}/{max_epochs} | \"\n",
    "#         f\"Train Loss: {train_loss:.5f}, Train R²: {train_r2:.4f} | \"\n",
    "#         f\"Val Loss: {val_loss:.5f}, Val R²: {val_r2:.4f} | \"\n",
    "#         f\"LR: {curr_lr}, Time: {round(elapsed, 2)} secs\"\n",
    "#     )\n",
    "\n",
    "# # Ensure model is saved even if no improvement occurred\n",
    "# if not model_save_path.exists():\n",
    "#     torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# # Save training logs for analysis and visualization\n",
    "# train_val_df = pd.DataFrame(train_val_logs)\n",
    "# train_val_df.to_csv(logs_save_path, index=False)\n",
    "# print(\"Training Complete....!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f67e20e",
   "metadata": {},
   "source": [
    "### Training/Validation Loss Visualization\n",
    "This section creates a comprehensive loss curve plot to monitor training progress and identify potential overfitting or convergence issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ef42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training logs and create loss curve visualization\n",
    "df_log = pd.read_csv(logs_save_path)\n",
    "\n",
    "# Create loss curve plot with training and validation losses\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(x=\"epoch\", y=\"train_loss\", data=df_log, label=\"Train Loss\")\n",
    "sns.lineplot(x=\"epoch\", y=\"val_loss\", data=df_log, linestyle=\"--\", label=\"Val Loss\")\n",
    "\n",
    "# Enhance plot readability and save\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(output_dir / f\"{target_col}_losscurve.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4635212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test DataLoader without shuffling for consistent evaluation\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test, y_test),\n",
    "    num_workers=4,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    batch_size=b_sz,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef04080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on test data with multiple regression metrics\n",
    "mae, rmse, nrmse, r2 = evaluate.evaluate_model(\n",
    "    model,\n",
    "    data_loader=test_loader,\n",
    "    tgt=target_col,\n",
    "    device=device,\n",
    "    scaler_path=scaler_save_path,\n",
    ")\n",
    "\n",
    "# Display comprehensive evaluation metrics\n",
    "print(f\"\\n\\nMean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Root Mean Square Error: {rmse:.4f}\")\n",
    "print(f\"Normalized Root Mean Square Error: {nrmse:.4f}\")\n",
    "print(f\"Coefficient of determination (R-Squared): {r2:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e31cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "potsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
