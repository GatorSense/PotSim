{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d607d44-b7a6-4302-b73c-e9cfcaf6b1d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:21:56.290663Z",
     "iopub.status.busy": "2025-01-31T22:21:56.290540Z",
     "iopub.status.idle": "2025-01-31T22:22:27.694105Z",
     "shell.execute_reply": "2025-01-31T22:22:27.693609Z",
     "shell.execute_reply.started": "2025-01-31T22:21:56.290647Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import potsimloader as psl\n",
    "import models as mdl\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.multiprocessing as mp\n",
    "from torchinfo import summary\n",
    "from torchmetrics.regression import R2Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d639d061-7d7f-4ddd-af0d-2215594f45a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:22:27.696338Z",
     "iopub.status.busy": "2025-01-31T22:22:27.696065Z",
     "iopub.status.idle": "2025-01-31T22:22:27.700280Z",
     "shell.execute_reply": "2025-01-31T22:22:27.699914Z",
     "shell.execute_reply.started": "2025-01-31T22:22:27.696322Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_FEATURES = {\n",
    "    \"NTotL1\": [\"DayAfterPlant\", \"NApp\", \"Rain\", \"SolarRad\", \"AirTempC\"],\n",
    "    \"NTotL2\": [\"DayAfterPlant\", \"NApp\", \"Rain\", \"SolarRad\", \"AirTempC\"],\n",
    "    \"SWatL1\": ['DayAfterPlant','IrrgThresh', 'NLeach', 'Irrg', 'Rain'],\n",
    "    \"SWatL2\": ['DayAfterPlant','IrrgThresh', 'NLeach', 'Irrg', 'Rain'],\n",
    "    \"NLeach\": ['NTotL1', 'NTotL2', 'Rain', 'SolarRad', 'AirTempC'],\n",
    "    \"NPlantUp\": ['DayAfterPlant', 'NTotL1', 'NTotL2', 'Rain', 'SolarRad', 'AirTempC']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06214f0e-d709-4086-9360-7d84bc8c401b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:22:27.706446Z",
     "iopub.status.busy": "2025-01-31T22:22:27.706328Z",
     "iopub.status.idle": "2025-01-31T22:22:27.717604Z",
     "shell.execute_reply": "2025-01-31T22:22:27.717215Z",
     "shell.execute_reply.started": "2025-01-31T22:22:27.706433Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)  # Set seed for PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # Set seed for PyTorch GPU (if using CUDA)\n",
    "    np.random.seed(seed)  # Set seed for NumPy\n",
    "    random.seed(seed)  # Set seed for Python's random module\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b64630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocessing as ppsr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d50fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_treatments(n_values):\n",
    "    combis = list(itertools.product(n_values, repeat=3))\n",
    "    return ['-'.join(map(str, t)) for t in combis if sum(t) <= 400]\n",
    "\n",
    "dssat_file = \"data/potsim_yearly/potsim_2001.parquet\"\n",
    "weather_file = \"data/weather.parquet\"\n",
    "\n",
    "n_values = [0, 56, 112, 168, 196]\n",
    "# n_values = [28, 84, 140]\n",
    "treatments = generate_treatments(n_values)\n",
    "\n",
    "scenarios = {\n",
    "    \"Year\": list(range(2004, 2024)),\n",
    "    \"Treatment\": treatments,\n",
    "    \"PlantingDay\": [1, 15, 29, 43, 57],\n",
    "    # \"IrrgDep\": [30, 50],\n",
    "    # \"IrrgThresh\": [50, 70, 90]\n",
    "}\n",
    "# scenarios = {\n",
    "#     \"Year\": list(range(2018, 2024)),\n",
    "#     \"Treatment\": treatments,\n",
    "#     \"PlantingDay\": [8, 22, 36, 50, 64],\n",
    "#     \"IrrgDep\": [40, 60],\n",
    "#     \"IrrgThresh\": [60, 80, 100]\n",
    "# }\n",
    "usecols = ['Year', 'Date', 'Treatment', 'NFirstApp','PlantingDay', 'IrrgDep',\n",
    "           'IrrgThresh', 'DayAfterPlant', 'NApp', 'NLeach', 'NPlantUp', 'NTotL1', \n",
    "           'NTotL2', 'Irrg', 'SWatL1', 'SWatL2', 'Rain', 'SolarRad', 'AirTempC']\n",
    "    \n",
    "mask = (\n",
    "    ((pl.col(\"NFirstApp\") == \"Npl\") & (pl.col(\"DayAfterPlant\") >= -1)) |\n",
    "    ((pl.col(\"NFirstApp\") == \"Npre\") & (pl.col(\"DayAfterPlant\") >= -37))\n",
    ")\n",
    "\n",
    "data = psl.read_data(\n",
    "    dataset_path = dssat_file,\n",
    "    weather_path = weather_file,\n",
    "    usecols = usecols,\n",
    "    scenarios=scenarios,\n",
    "    lazy = False\n",
    ")\n",
    "df = data.filter(mask).to_pandas()\n",
    "numcols = df.select_dtypes(exclude=['category']).columns\n",
    "df[numcols] = df[numcols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d033fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "process_data() missing 4 required positional arguments: 'df', 'feats', 'tgt', and 'scaler_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m x,y= \u001b[43mppsr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: process_data() missing 4 required positional arguments: 'df', 'feats', 'tgt', and 'scaler_path'"
     ]
    }
   ],
   "source": [
    "\n",
    "x,y= ppsr.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9526856d-12a1-4bb6-9c76-cba3ab8c2f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:22:27.718324Z",
     "iopub.status.busy": "2025-01-31T22:22:27.718070Z",
     "iopub.status.idle": "2025-01-31T22:22:27.727458Z",
     "shell.execute_reply": "2025-01-31T22:22:27.727040Z",
     "shell.execute_reply.started": "2025-01-31T22:22:27.718310Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(df, feature_cols, target_col, seq_len=None):\n",
    "    scenario_vars = ['Year', 'PlantingDay', 'Treatment', 'NFirstApp', 'OrgIrrgDep', 'OrgIrrgThresh']\n",
    "    df_sorted = df.sort_values(scenario_vars + ['OrgDayAfterPlant'])\n",
    "    data = df_sorted[feature_cols].values\n",
    "    target = df_sorted[target_col].values\n",
    "    \n",
    "    # Empty Vectorized window creation in case no data\n",
    "    if data.size == 0 or len(data) < seq_len:\n",
    "        return np.empty((0, seq_len, len(feature_cols))), np.empty(0)\n",
    "    \n",
    "    # Create group boundaries using vectorized operations\n",
    "    group_mask = (df_sorted[scenario_vars] != df_sorted[scenario_vars].shift()).any(axis=1).values\n",
    "    group_ids = np.cumsum(group_mask)\n",
    "    \n",
    "    # Create validity mask for full sequences within groups\n",
    "    valid = np.zeros(len(data) - seq_len + 1, dtype=bool)\n",
    "    for i in range(len(valid)):\n",
    "        valid[i] = group_ids[i] == group_ids[i + seq_len - 1]\n",
    "    \n",
    "    # Create windows and transpose dimensions to [samples, seq_len, features]\n",
    "    X = sliding_window_view(data, (seq_len,), axis=0)[valid].transpose(0, 2, 1)\n",
    "    y = target[seq_len-1:][valid]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def transform_data(df, feature_cols, target_col, scaler_path, mode=\"transform\"):\n",
    "    scenario_vars = ['Year', 'PlantingDay', 'Treatment', 'NFirstApp', 'OrgIrrgDep', 'OrgIrrgThresh']  \n",
    "    df_copy = df.copy()\n",
    "    df_copy['OrgDayAfterPlant'] = df['DayAfterPlant']\n",
    "    df_copy['OrgIrrgDep'] = df['IrrgDep']\n",
    "    df_copy['OrgIrrgThresh'] = df['IrrgThresh']\n",
    "    df_copy = df_copy.sort_values(by=scenario_vars + ['OrgDayAfterPlant'])\n",
    "    df_copy[\"NApp\"] = df_copy.groupby(scenario_vars, observed=True)[\"NApp\"].transform('cumsum')\n",
    "    df_copy, _ = mdl.normalize_columns(\n",
    "        df_copy, feature_cols + [target_col],\n",
    "        mode=mode, scaler_path=scaler_path\n",
    "    )\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "\n",
    "def process_data(df, feature_cols, target_col, scaler_path, mode=\"transform\", seq_len=None):\n",
    "    if not all(col in df.columns for col in feature_cols + [target_col]):\n",
    "        raise ValueError(\"Some columns not found in dataframe\")\n",
    "        \n",
    "    df_copy =  transform_data(df, feature_cols, target_col, scaler_path, mode=mode)\n",
    "    # For non-sequential data preparation\n",
    "    if seq_len is None:\n",
    "        X = df_copy[feature_cols].values\n",
    "        y = df_copy[target_col].values\n",
    "        del df_copy\n",
    "        return torch.FloatTensor(X), torch.FloatTensor(y).view(-1, 1)\n",
    "    \n",
    "    # For sequence preparation\n",
    "    X, y = prepare_sequences(df_copy, feature_cols, target_col, seq_len=seq_len)\n",
    "    del df_copy\n",
    "    return torch.FloatTensor(X), torch.FloatTensor(y).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c045c3d-5dcd-4838-8582-01710761ad99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:22:27.728186Z",
     "iopub.status.busy": "2025-01-31T22:22:27.727927Z",
     "iopub.status.idle": "2025-01-31T22:22:27.738924Z",
     "shell.execute_reply": "2025-01-31T22:22:27.738557Z",
     "shell.execute_reply.started": "2025-01-31T22:22:27.728171Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_train_val_test(df):\n",
    "    # 1. Define year splits\n",
    "    train_years = list(range(2004, 2016))  # 12 years\n",
    "    val_years = list(range(2016, 2020))    # 4 years\n",
    "    test_years = list(range(2020, 2024))    # 4 years\n",
    "    \n",
    "    train_data = df[df[\"Year\"].isin(train_years)]\n",
    "    val_data = df[df[\"Year\"].isin(val_years)]\n",
    "    test_data = df[df[\"Year\"].isin(test_years)]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def random_sample_train_val_test(df):    \n",
    "    scenario_vars = ['PlantingDay', 'Treatment', 'NFirstApp', 'IrrgDep', 'IrrgThresh']\n",
    "    groups = df.groupby(scenario_vars, observed=True, sort=False)\n",
    "    unique_groups = list(groups.groups.keys())\n",
    "    np.random.shuffle(unique_groups)\n",
    "    \n",
    "    # Shuffle and split 70-15-15\n",
    "    n = len(unique_groups)\n",
    "    n_train = int(0.6 * n)\n",
    "    n_val = int(0.2 * n)\n",
    "    print(f\"Scenarios: train({n_train * 12}), val({n_val * 4}), test({(n - n_train - n_val) * 4})\")\n",
    "\n",
    "    train_set = unique_groups[:n_train]\n",
    "    val_set = unique_groups[n_train:n_train + n_val]\n",
    "    test_set = unique_groups[n_train + n_val:]\n",
    "    \n",
    "    # Split data between train, val, test fro years to reduce matching\n",
    "    train_data, val_data, test_data = split_train_val_test(df)\n",
    "    \n",
    "    def _filter_scenarios(_data, _dset):\n",
    "        _dset = pd.DataFrame(_dset, columns=scenario_vars)\n",
    "        for col in scenario_vars:\n",
    "            if _data[col].dtype.name == 'category':\n",
    "                _dset[col] = _dset[col].astype(_data[col].dtype)\n",
    "        return _data.merge(_dset, on=scenario_vars, how='inner')\n",
    "    \n",
    "    return (_filter_scenarios(train_data, train_set), \n",
    "            _filter_scenarios(val_data, val_set), \n",
    "            _filter_scenarios(test_data, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b402d55d-83d4-4d64-9bff-bfcc67a73d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:22:27.739715Z",
     "iopub.status.busy": "2025-01-31T22:22:27.739369Z",
     "iopub.status.idle": "2025-01-31T22:23:19.467833Z",
     "shell.execute_reply": "2025-01-31T22:23:19.467337Z",
     "shell.execute_reply.started": "2025-01-31T22:22:27.739701Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: data\\dssat_scenarios_latest.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     26\u001b[39m usecols = [\u001b[33m'\u001b[39m\u001b[33mYear\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTreatment\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNFirstApp\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mPlantingDay\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mIrrgDep\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     27\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mIrrgThresh\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDayAfterPlant\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNApp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNLeach\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNPlantUp\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNTotL1\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     28\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mNTotL2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mIrrg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSWatL1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSWatL2\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mRain\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSolarRad\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAirTempC\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     30\u001b[39m mask = (\n\u001b[32m     31\u001b[39m     ((pl.col(\u001b[33m\"\u001b[39m\u001b[33mNFirstApp\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mNpl\u001b[39m\u001b[33m\"\u001b[39m) & (pl.col(\u001b[33m\"\u001b[39m\u001b[33mDayAfterPlant\u001b[39m\u001b[33m\"\u001b[39m) >= -\u001b[32m1\u001b[39m)) |\n\u001b[32m     32\u001b[39m     ((pl.col(\u001b[33m\"\u001b[39m\u001b[33mNFirstApp\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mNpre\u001b[39m\u001b[33m\"\u001b[39m) & (pl.col(\u001b[33m\"\u001b[39m\u001b[33mDayAfterPlant\u001b[39m\u001b[33m\"\u001b[39m) >= -\u001b[32m37\u001b[39m))\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m data = \u001b[43mpsl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdssat_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweather_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mweather_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscenarios\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscenarios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m df = data.filter(mask).to_pandas()\n\u001b[32m     43\u001b[39m numcols = df.select_dtypes(exclude=[\u001b[33m'\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m'\u001b[39m]).columns\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\spothapragada\\Documents\\AI_Harvest\\PotSim\\potsimloader.py:276\u001b[39m, in \u001b[36mread_data\u001b[39m\u001b[34m(dataset_path, weather_path, usecols, scenarios, lazy, as_pandas)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Reads and processes data from files with optional filtering and column selection.\u001b[39;00m\n\u001b[32m    258\u001b[39m \n\u001b[32m    259\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    271\u001b[39m \u001b[33;03m    ValueError: If specified columns are not found.\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m _POTSIM_PATH, _POTSIM_FRAME\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m data = \u001b[43m_scan_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m _POTSIM_PATH = Path(dataset_path)\n\u001b[32m    278\u001b[39m _POTSIM_FRAME = data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\spothapragada\\Documents\\AI_Harvest\\PotSim\\potsimloader.py:118\u001b[39m, in \u001b[36m_scan_data\u001b[39m\u001b[34m(filepath)\u001b[39m\n\u001b[32m    116\u001b[39m filepath = Path(filepath)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filepath.is_file():\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m readers = {\n\u001b[32m    121\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m: pl.scan_csv,\n\u001b[32m    122\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m.parquet\u001b[39m\u001b[33m'\u001b[39m: pl.scan_parquet\n\u001b[32m    123\u001b[39m }\n\u001b[32m    125\u001b[39m reader = readers.get(filepath.suffix)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: File not found: data\\dssat_scenarios_latest.parquet"
     ]
    }
   ],
   "source": [
    "def generate_treatments(n_values):\n",
    "    combis = list(itertools.product(n_values, repeat=3))\n",
    "    return ['-'.join(map(str, t)) for t in combis if sum(t) <= 400]\n",
    "\n",
    "dssat_file = \"data/dssat_scenarios_latest.parquet\"\n",
    "weather_file = \"data/weather_2000-2023.parquet\"\n",
    "\n",
    "n_values = [0, 56, 112, 168, 196]\n",
    "# n_values = [28, 84, 140]\n",
    "treatments = generate_treatments(n_values)\n",
    "\n",
    "scenarios = {\n",
    "    \"Year\": list(range(2004, 2024)),\n",
    "    \"Treatment\": treatments,\n",
    "    \"PlantingDay\": [1, 15, 29, 43, 57],\n",
    "    # \"IrrgDep\": [30, 50],\n",
    "    # \"IrrgThresh\": [50, 70, 90]\n",
    "}\n",
    "# scenarios = {\n",
    "#     \"Year\": list(range(2018, 2024)),\n",
    "#     \"Treatment\": treatments,\n",
    "#     \"PlantingDay\": [8, 22, 36, 50, 64],\n",
    "#     \"IrrgDep\": [40, 60],\n",
    "#     \"IrrgThresh\": [60, 80, 100]\n",
    "# }\n",
    "usecols = ['Year', 'Date', 'Treatment', 'NFirstApp','PlantingDay', 'IrrgDep',\n",
    "           'IrrgThresh', 'DayAfterPlant', 'NApp', 'NLeach', 'NPlantUp', 'NTotL1', \n",
    "           'NTotL2', 'Irrg', 'SWatL1', 'SWatL2', 'Rain', 'SolarRad', 'AirTempC']\n",
    "    \n",
    "mask = (\n",
    "    ((pl.col(\"NFirstApp\") == \"Npl\") & (pl.col(\"DayAfterPlant\") >= -1)) |\n",
    "    ((pl.col(\"NFirstApp\") == \"Npre\") & (pl.col(\"DayAfterPlant\") >= -37))\n",
    ")\n",
    "\n",
    "data = psl.read_data(\n",
    "    dataset_path = dssat_file,\n",
    "    weather_path = weather_file,\n",
    "    usecols = usecols,\n",
    "    scenarios=scenarios,\n",
    "    lazy = False\n",
    ")\n",
    "df = data.filter(mask).to_pandas()\n",
    "numcols = df.select_dtypes(exclude=['category']).columns\n",
    "df[numcols] = df[numcols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92abeb87-21b3-400a-b3dc-5b9faf2dd21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:23:19.468500Z",
     "iopub.status.busy": "2025-01-31T22:23:19.468364Z",
     "iopub.status.idle": "2025-01-31T22:23:19.640488Z",
     "shell.execute_reply": "2025-01-31T22:23:19.640019Z",
     "shell.execute_reply.started": "2025-01-31T22:23:19.468487Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56246400 entries, 0 to 56246399\n",
      "Data columns (total 19 columns):\n",
      " #   Column         Dtype         \n",
      "---  ------         -----         \n",
      " 0   Year           int32         \n",
      " 1   Date           datetime64[ms]\n",
      " 2   Treatment      category      \n",
      " 3   NFirstApp      category      \n",
      " 4   PlantingDay    int32         \n",
      " 5   IrrgDep        int32         \n",
      " 6   IrrgThresh     int32         \n",
      " 7   DayAfterPlant  int32         \n",
      " 8   NApp           float32       \n",
      " 9   NLeach         float32       \n",
      " 10  NPlantUp       float32       \n",
      " 11  NTotL1         float32       \n",
      " 12  NTotL2         float32       \n",
      " 13  Irrg           float32       \n",
      " 14  SWatL1         float32       \n",
      " 15  SWatL2         float32       \n",
      " 16  Rain           float32       \n",
      " 17  SolarRad       float32       \n",
      " 18  AirTempC       float32       \n",
      "dtypes: category(2), datetime64[ms](1), float32(11), int32(5)\n",
      "memory usage: 3.9 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b84b0efd-91be-4bf8-85c9-21ca9c17b8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:23:19.641264Z",
     "iopub.status.busy": "2025-01-31T22:23:19.640990Z",
     "iopub.status.idle": "2025-01-31T22:23:19.918391Z",
     "shell.execute_reply": "2025-01-31T22:23:19.917914Z",
     "shell.execute_reply.started": "2025-01-31T22:23:19.641247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_col = \"NTotL1\"\n",
    "feature_cols = TARGET_FEATURES[target_col]\n",
    "seq_len = None\n",
    "lr = 0.005\n",
    "b_sz = 2048\n",
    "max_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5cf65c-191f-4b02-939c-6d6be7ffa972",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:23:19.919163Z",
     "iopub.status.busy": "2025-01-31T22:23:19.918896Z",
     "iopub.status.idle": "2025-01-31T22:23:19.929054Z",
     "shell.execute_reply": "2025-01-31T22:23:19.928714Z",
     "shell.execute_reply.started": "2025-01-31T22:23:19.919148Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = \"EncoderOnlyTransformer\"\n",
    "# model = mdl.EncoderOnlyTransformer(\n",
    "#         input_dim=len(feature_cols), num_layers=2, nhead=4,\n",
    "#         d_model=128, max_seq_len=seq_len)\n",
    "# model_name=\"CNN1D\"\n",
    "# model = mdl.CNN1D(input_dim=len(feature_cols), hidden_size=128)\n",
    "\n",
    "# model_name = \"MLP\"\n",
    "# model = mdl.MLP(input_dim=len(feature_cols), \n",
    "#                        hidden_size=128, \n",
    "#                        num_layers=2, \n",
    "                       # dropout=0.2)\n",
    "\n",
    "# model_name= \"TCN\"\n",
    "# model = mdl.TCN(input_dim=len(feature_cols), \n",
    "#                         num_channels=[128, 64, 32, 16, 8], \n",
    "#                         kernel_size=3)\n",
    "\n",
    "# model_name=\"Linear_Regression\"\n",
    "# model = mdl.LinearRegression(input_dim=len(feature_cols))\n",
    "\n",
    "# model_name=\"LSTM\"\n",
    "# model = mdl.LSTM(input_dim=len(feature_cols), num_layers=2, hidden_size=128)\n",
    "\n",
    "# model.to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-8)\n",
    "# ampscaler = torch.amp.GradScaler(device=device)\n",
    "scaler_path=f\"models/notebook/{target_col}_scaler_v3.pkl\"\n",
    "# print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7569fdca-1968-4bf2-a42b-b5e9337bcef6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:23:19.929557Z",
     "iopub.status.busy": "2025-01-31T22:23:19.929437Z",
     "iopub.status.idle": "2025-01-31T22:23:57.481003Z",
     "shell.execute_reply": "2025-01-31T22:23:57.480536Z",
     "shell.execute_reply.started": "2025-01-31T22:23:19.929546Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenarios: train(160704), val(17856), test(17856)\n",
      "torch.Size([20267712, 5]) torch.Size([20267712, 1]) torch.Size([2246256, 5]) torch.Size([2246256, 1]) torch.Size([2247120, 5]) torch.Size([2247120, 1])\n"
     ]
    }
   ],
   "source": [
    "# Strategy-1 (equal proportions)\n",
    "# train_split, val_split, test_split = split_train_val_test(df)\n",
    "\n",
    "# Strategy-2 (random sampling)\n",
    "train_split, val_split, test_split = random_sample_train_val_test(df)\n",
    "\n",
    "# # Process Data\n",
    "X_train, y_train = process_data(train_split, feature_cols, target_col, \n",
    "                                scaler_path=scaler_path, mode=\"fit\", seq_len=seq_len)\n",
    "X_val, y_val = process_data(val_split, feature_cols, target_col,\n",
    "                            scaler_path=scaler_path, mode=\"transform\", seq_len=seq_len)\n",
    "X_test, y_test = process_data(test_split, feature_cols, target_col,\n",
    "                              scaler_path=scaler_path, mode=\"transform\", seq_len=seq_len)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a871731e-0925-495b-90d7-4394e42d2181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:24:57.095611Z",
     "iopub.status.busy": "2025-01-31T22:24:57.095250Z",
     "iopub.status.idle": "2025-01-31T22:24:57.492881Z",
     "shell.execute_reply": "2025-01-31T22:24:57.492598Z",
     "shell.execute_reply.started": "2025-01-31T22:24:57.095597Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.23],\n",
       "       [9.3 ],\n",
       "       [9.37],\n",
       "       [9.44],\n",
       "       [9.51],\n",
       "       [9.57],\n",
       "       [9.63],\n",
       "       [9.69],\n",
       "       [9.75],\n",
       "       [9.81]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_normalize(y_train[:10], target_col, scaler_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ea5ab8-9a9a-4c77-aaf4-1ed9cc66145f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T08:07:11.902931Z",
     "iopub.status.busy": "2025-01-31T08:07:11.902717Z",
     "iopub.status.idle": "2025-01-31T08:07:11.905800Z",
     "shell.execute_reply": "2025-01-31T08:07:11.905476Z",
     "shell.execute_reply.started": "2025-01-31T08:07:11.902916Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TensorDataset(X_train, y_train),\n",
    "                          batch_size=b_sz, shuffle=True, \n",
    "                          num_workers=10, persistent_workers=True, \n",
    "                          prefetch_factor=4, pin_memory=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val),\n",
    "                          batch_size=b_sz, shuffle=False, \n",
    "                          num_workers=4, persistent_workers=True, \n",
    "                          prefetch_factor=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92eeec1a-0827-45d9-abe9-40976ccf1cc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:24:52.911582Z",
     "iopub.status.busy": "2025-01-31T22:24:52.911372Z",
     "iopub.status.idle": "2025-01-31T22:24:52.915199Z",
     "shell.execute_reply": "2025-01-31T22:24:52.914914Z",
     "shell.execute_reply.started": "2025-01-31T22:24:52.911568Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred) \n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "def inverse_normalize(arr, target_col, scaler_path):\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        if target_col not in params:\n",
    "            raise ValueError(f'{taregt_col} not present in scaler file.')\n",
    "        min_val = params[target_col]['min']\n",
    "        max_val = params[target_col]['max']\n",
    "        return (np.array(arr) * (max_val - min_val)) + min_val\n",
    "    \n",
    "def get_scaler_min_max(target_col, scaler_path, device):\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        if target_col not in params:\n",
    "            raise ValueError(f'{taregt_col} not present in scaler file.')\n",
    "        min_val = torch.tensor(params[target_col]['min'], device=device)\n",
    "        max_val = torch.tensor(params[target_col]['max'], device=device)\n",
    "        return min_val, max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2bf473c-ca14-4543-a344-6e5c683436f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T08:07:11.920206Z",
     "iopub.status.busy": "2025-01-31T08:07:11.920019Z",
     "iopub.status.idle": "2025-01-31T08:07:11.933027Z",
     "shell.execute_reply": "2025-01-31T08:07:11.932719Z",
     "shell.execute_reply.started": "2025-01-31T08:07:11.920193Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, \n",
    "                criterion, device, \n",
    "                min_val, max_val,\n",
    "                ampscaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    r2_metric = R2Score().to(device)\n",
    "    for inputs, targets in loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        if ampscaler:\n",
    "            with torch.amp.autocast(device_type=\"cuda\"):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            ampscaler.scale(loss).backward()\n",
    "            # ampscaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            ampscaler.step(optimizer)\n",
    "            ampscaler.update()\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        outputs_dn = (outputs * (max_val - min_val)) + min_val\n",
    "        targets_dn = (targets * (max_val - min_val)) + min_val\n",
    "        r2_metric.update(outputs_dn.float(), targets_dn.float())\n",
    "        total_loss += loss.float().item()\n",
    "    r2 = r2_metric.compute().cpu().item()\n",
    "    r2_metric.reset()\n",
    "    return total_loss / len(loader), r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "712a9b1b-5957-4b7c-8094-b3402a8e2a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T08:07:11.933556Z",
     "iopub.status.busy": "2025-01-31T08:07:11.933365Z",
     "iopub.status.idle": "2025-01-31T08:07:11.943271Z",
     "shell.execute_reply": "2025-01-31T08:07:11.942967Z",
     "shell.execute_reply.started": "2025-01-31T08:07:11.933542Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def val_epoch(model, loader, criterion, device, min_val, max_val):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    r2_metric = R2Score().to(device)\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            with torch.amp.autocast(device_type=device.type):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            outputs_dn = (outputs * (max_val - min_val)) + min_val\n",
    "            targets_dn = (targets * (max_val - min_val)) + min_val\n",
    "            r2_metric.update(outputs, targets)\n",
    "            total_loss += loss.float().item()\n",
    "    r2 = r2_metric.compute().cpu().item()\n",
    "    r2_metric.reset()\n",
    "    return total_loss / len(loader), r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa398a8-d1fb-49f2-86a2-0b0b3f6cc576",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T08:07:11.944087Z",
     "iopub.status.busy": "2025-01-31T08:07:11.943766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP on device: cuda\n",
      "Epoch: 0/9 | Train Loss: 0.01124, Train R²: 0.7503 | Val Loss: 0.00736, Val R²: 0.8410 | LR: 0.005, Time: 39.36 secs\n",
      "Epoch: 1/9 | Train Loss: 0.00877, Train R²: 0.8051 | Val Loss: 0.00716, Val R²: 0.8452 | LR: 0.005, Time: 38.75 secs\n",
      "Epoch: 2/9 | Train Loss: 0.00838, Train R²: 0.8139 | Val Loss: 0.00687, Val R²: 0.8516 | LR: 0.005, Time: 38.93 secs\n",
      "Epoch: 3/9 | Train Loss: 0.00814, Train R²: 0.8193 | Val Loss: 0.00685, Val R²: 0.8520 | LR: 0.005, Time: 41.64 secs\n",
      "Epoch: 4/9 | Train Loss: 0.00794, Train R²: 0.8237 | Val Loss: 0.00669, Val R²: 0.8555 | LR: 0.005, Time: 43.22 secs\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "train_val_logs = []\n",
    "early_stop = False\n",
    "epochs_no_improve = 0\n",
    "early_stop_patience = 10\n",
    "min_tgt_val, max_tgt_val = get_scaler_min_max(target_col, scaler_path, device)\n",
    "print(f\"Training {model_name} on device: {device}\")\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(max_epochs):\n",
    "    if early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_r2 = train_epoch(model, train_loader, optimizer, \n",
    "                                       criterion, device, min_tgt_val, max_tgt_val, \n",
    "                                       ampscaler=ampscaler)\n",
    "    val_loss, val_r2 = val_epoch(model, val_loader, criterion, device, min_tgt_val, max_tgt_val)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stop and save logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"models/notebook/{target_col}_{model_name}_v3.pth\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve > early_stop_patience:\n",
    "            early_stop = True\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    train_val_logs.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"val_r2\": val_r2,\n",
    "        \"learning_rate\": curr_lr\n",
    "    })\n",
    "    print(f\"Epoch: {epoch}/{max_epochs-1} | \" \n",
    "          f\"Train Loss: {train_loss:.5f}, Train R²: {train_r2:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.5f}, Val R²: {val_r2:.4f} | \"\n",
    "          f\"LR: {curr_lr}, Time: {round(elapsed, 2)} secs\")\n",
    "\n",
    "train_val_df = pd.DataFrame(train_val_logs)\n",
    "train_val_df.to_csv(f\"models/notebook/{target_col}_{model_name}_train_val_logs_v3.csv\", index=False)\n",
    "\n",
    "# Load best model weights before returning\n",
    "model.load_state_dict(torch.load(f\"models/notebook/{target_col}_{model_name}_v3.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeceee6-ad4e-4812-957f-89ad46ada1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_log = pd.read_csv(f\"models/notebook/{target_col}_{model_name}_train_val_logs_v3.csv\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(x=\"epoch\", y=\"train_loss\", data=df_log, label=\"Train Loss\")\n",
    "sns.lineplot(x=\"epoch\", y=\"val_loss\", data=df_log, linestyle=\"--\", label=\"Val Loss\")\n",
    "plt.title(f'Training and Validation Loss (Strategy-3) {model_name}, {target_col}')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "# plt.savefig(f'{target_col}_yb_loss_curve.png', dpi=100, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c13c536d-7136-49a2-8610-5a97c2223beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T23:14:34.137546Z",
     "iopub.status.busy": "2025-01-31T23:14:34.137266Z",
     "iopub.status.idle": "2025-01-31T23:14:34.161004Z",
     "shell.execute_reply": "2025-01-31T23:14:34.160678Z",
     "shell.execute_reply.started": "2025-01-31T23:14:34.137530Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scenario(tdata, scenario):\n",
    "    return tdata[\n",
    "        (tdata['Treatment'] == scenario['Treatment']) &\n",
    "        (tdata['NFirstApp'] == scenario['NFirstApp']) &\n",
    "        (tdata['PlantingDay'] == scenario['PlantingDay']) &\n",
    "        (tdata['Year'] == scenario['Year']) &\n",
    "        (tdata['IrrgDep'] == scenario['IrrgDep']) &\n",
    "        (tdata['IrrgThresh'] == scenario['IrrgThresh']) \n",
    "    ].sort_values(\"DayAfterPlant\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def evaluate_and_plot(df, model_name, model, scenario,\n",
    "                     feature_cols, target_col, scaler_path, seq_len=None):\n",
    "    sc_df = get_scenario(df, scenario)\n",
    "    original_dap = sc_df['DayAfterPlant'].values\n",
    "    sc_df =  preserve_original_columns(sc_df)\n",
    "    sc_df = cumulate_columns(sc_df, \"NApp\")\n",
    "    sc_df_norm, _ = mdl.normalize_columns(\n",
    "        sc_df, \n",
    "        feature_cols + [target_col],\n",
    "        mode=\"transform\", \n",
    "        scaler_path=scaler_path\n",
    "    )\n",
    "    X_tensor, _ = process_data(sc_df_norm, feature_cols, target_col, seq_len=seq_len)\n",
    "    y_true = sc_df[target_col].values\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "    # Step 6: De-normalize predictions\n",
    "    pred_df = pd.DataFrame({\n",
    "        target_col: y_pred,\n",
    "        'DayAfterPlant': original_dap[seq_len-1:] if seq_len != None else original_dap\n",
    "    })\n",
    "    pred_df = mdl.denormalize_columns(\n",
    "        pred_df, \n",
    "        [target_col], \n",
    "        scaler_path=scaler_path\n",
    "    )\n",
    "    y_pred = pred_df[target_col].values\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    ax1.plot(original_dap, y_true, label=f'True {target_col}', color='blue')\n",
    "    if seq_len is None:\n",
    "        ax1.plot(original_dap, y_pred, label=f'Predicted {target_col}', color='red')\n",
    "    else:\n",
    "        ax1.plot(original_dap[seq_len-1:], y_pred, label=f'Predicted {target_col}', color='red')\n",
    "    ax1.set_xlabel('DayAfterPlant')\n",
    "    ax1.set_ylabel(target_col)\n",
    "    # ax1.set_ylim(0, 100)\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(original_dap, sc_df['Rain'], label='Rain', color='lightgreen', linestyle='--', alpha=0.5)\n",
    "    ax2.set_ylabel('Rain (mm)', color='lightgreen')\n",
    "    ax2.set_ylim(0, 50)\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "#     # Add N application markers\n",
    "    n_first_app = scenario['NFirstApp']\n",
    "    if n_first_app == \"Npre\":\n",
    "        app_daps = [-36, 22, 44]  # Pre-plant, emergence, tillering\n",
    "    elif n_first_app == \"Npl\":\n",
    "        app_daps = [0, 22, 44]    # Planting, emergence, tillering\n",
    "    else:\n",
    "        app_daps = []\n",
    "    for dap in app_daps:\n",
    "        if dap in original_dap:  # Check if DAP exists in scenario data\n",
    "            ax1.axvline(x=dap, color='gray', linestyle='--', linewidth=2.0, alpha=0.8)\n",
    "            ax1.text(dap, ax1.get_ylim()[1], f'N\\n↓', \n",
    "                     ha='center', va='bottom', color='gray')\n",
    "    \n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.title(f'Scenario: {scenario}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5a291d8-6bab-423e-ba5c-5041240c6bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T23:14:36.477685Z",
     "iopub.status.busy": "2025-01-31T23:14:36.477452Z",
     "iopub.status.idle": "2025-01-31T23:14:36.812848Z",
     "shell.execute_reply": "2025-01-31T23:14:36.812346Z",
     "shell.execute_reply.started": "2025-01-31T23:14:36.477670Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/notebook/NTotL1_Linear_Regression_v3.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinear_Regression\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m mdl\u001b[38;5;241m.\u001b[39mLinearRegression(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(feature_cols))\n\u001b[0;32m---> 16\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/notebook/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_col\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_v3.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# model=mdl.MLP(input_dim=len(feature_cols), hidden_size=256, num_layers=4)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# model = mdl.TCN(input_dim=len(feature_cols), num_channels=[256, 128, 64, 32, 16, 8], kernel_size=5)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/blue/azare/prateekgoel/conda/envs/aitorch/lib/python3.12/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/blue/azare/prateekgoel/conda/envs/aitorch/lib/python3.12/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/blue/azare/prateekgoel/conda/envs/aitorch/lib/python3.12/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/notebook/NTotL1_Linear_Regression_v3.pth'"
     ]
    }
   ],
   "source": [
    "# model_name = \"LSTM\"\n",
    "# model = mdl.LSTM(input_dim=len(feature_cols), \n",
    "#                  num_layers=2, \n",
    "#                  hidden_size=128)\n",
    "# model_name = \"EncoderOnlyTransformer\"\n",
    "# model = mdl.EncoderOnlyTransformer(\n",
    "#         input_dim=len(feature_cols), num_layers=2,\n",
    "#         d_model=128, max_seq_len=seq_len)\n",
    "# model_name = \"MLP\"\n",
    "# model = mdl.MLP(input_dim=len(feature_cols), \n",
    "#                        hidden_size=256, \n",
    "#                        num_layers=2, \n",
    "#                        dropout=0.2)\n",
    "model_name=\"Linear_Regression\"\n",
    "model = mdl.LinearRegression(input_dim=len(feature_cols))\n",
    "model.load_state_dict(torch.load(f\"models/notebook/{target_col}_{model_name}_v3.pth\", weights_only=True))\n",
    "# model=mdl.MLP(input_dim=len(feature_cols), hidden_size=256, num_layers=4)\n",
    "# model = mdl.TCN(input_dim=len(feature_cols), num_channels=[256, 128, 64, 32, 16, 8], kernel_size=5)\n",
    "\n",
    "print(f\"Training {model_name} on device: {device}\")\n",
    "print(summary(model))\n",
    "scenario = {\n",
    "    'Treatment': '0-112-112',\n",
    "    'NFirstApp': 'Npre',\n",
    "    'PlantingDay': 15,\n",
    "    'Year': 2020,\n",
    "    'IrrgDep': 50,\n",
    "    'IrrgThresh': 70\n",
    "}\n",
    "# get_scenario(test_data, scenario)\n",
    "evaluate_and_plot(\n",
    "    df, model_name, model, \n",
    "    scenario, feature_cols,target_col,\n",
    "    scaler_path=f\"models/notebook/{target_col}_scaler_v1.pkl\",\n",
    "    seq_len=seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6e8953a-a4d4-415c-bcf2-57b756fb0e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:26:17.960354Z",
     "iopub.status.busy": "2025-01-31T22:26:17.959999Z",
     "iopub.status.idle": "2025-01-31T22:26:17.964572Z",
     "shell.execute_reply": "2025-01-31T22:26:17.964256Z",
     "shell.execute_reply.started": "2025-01-31T22:26:17.960337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, model, X_tensor, y_tensor, feature_cols, \n",
    "                   target_col, batch_size, device, \n",
    "                   scaler_path, seq_len=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # X_tensor, y_tensor = process_data(df, feature_cols, target_col, \n",
    "    #                             scaler_path=scaler_path, mode=\"transform\", seq_len=seq_len)\n",
    "    tensor_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size, shuffle=False)\n",
    "    with torch.inference_mode():\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Flatten and format\n",
    "            targets = targets.numpy().flatten()\n",
    "            outputs = outputs.cpu().numpy().flatten()\n",
    "\n",
    "            outputs_denorm = inverse_normalize(outputs, target_col, scaler_path=scaler_path)\n",
    "            targets_denorm = inverse_normalize(targets, target_col, scaler_path=scaler_path)\n",
    "            \n",
    "            # print(targets[:10], outputs[:10])\n",
    "            # print(targets_denorm[:10], outputs_denorm[:10])\n",
    "            # return\n",
    "            all_preds.append(outputs_denorm)\n",
    "            all_targets.append(targets_denorm)\n",
    "            pgrs = (i+1) * 20 // len(data_loader)\n",
    "            print(f\"{i+1}/{len(data_loader)}[{'=' * pgrs}>{' ' * (19 - pgrs)}]\", end='\\r')\n",
    "    # Concatenate all predictions and targets\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_targets)\n",
    "    return compute_metrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0686f57b-26f6-4fc5-88a2-57ba01c56b02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T05:40:04.549429Z",
     "iopub.status.busy": "2025-01-31T05:40:04.549138Z",
     "iopub.status.idle": "2025-01-31T05:40:04.560315Z",
     "shell.execute_reply": "2025-01-31T05:40:04.559978Z",
     "shell.execute_reply.started": "2025-01-31T05:40:04.549415Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_table = {}\n",
    "result_table['Strategy_3'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb797588-28f7-4d93-9f1b-b7139b696824",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T22:26:19.833479Z",
     "iopub.status.busy": "2025-01-31T22:26:19.833151Z",
     "iopub.status.idle": "2025-01-31T22:26:35.169847Z",
     "shell.execute_reply": "2025-01-31T22:26:35.169493Z",
     "shell.execute_reply.started": "2025-01-31T22:26:19.833465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.296642 21.999334 0.482450008392334\n"
     ]
    }
   ],
   "source": [
    "# Load trained MLP model\n",
    "# model_name = \"EncoderOnlyTransformer\"\n",
    "# model = mdl.EncoderOnlyTransformer(\n",
    "#         input_dim=len(feature_cols), num_layers=2,\n",
    "#         d_model=128, max_seq_len=seq_len)\n",
    "# model_name = \"MLP\"\n",
    "# model = mdl.MLP(input_dim=len(feature_cols), \n",
    "#                        hidden_size=256, \n",
    "#                        num_layers=2, \n",
    "#                        dropout=0.2)\n",
    "model_name=\"LinearRegression\"\n",
    "model = mdl.LinearRegression(input_dim=len(feature_cols))\n",
    "model.load_state_dict(torch.load(f\"models/{target_col}/{target_col}_{model_name}.pth\", weights_only=True))\n",
    "\n",
    "# result_table['Strategy_3'][target_col] = {}\n",
    "mae, rmse, r2 = evaluate_model(\n",
    "    model_name, model, X_val, y_val, \n",
    "    feature_cols, target_col,b_sz, device,\n",
    "    scaler_path=f\"models/{target_col}/{target_col}_scaler.pkl\", \n",
    "    seq_len=seq_len\n",
    ")\n",
    "# result_table['Strategy_3'][target_col][\"train\"] = {\n",
    "#     \"mae\": mae,\n",
    "#     \"rmse\": rmse,\n",
    "#     \"r2\": r2\n",
    "# }\n",
    "\n",
    "# mae, rmse, r2 = evaluate_model(\n",
    "#     model_name, model, val_split, \n",
    "#     feature_cols, target_col,b_sz, device,\n",
    "#     scaler_path=f\"models/notebook/{target_col}_scaler_v3.pkl\", \n",
    "#     seq_len=seq_len\n",
    "# )\n",
    "# result_table['Strategy_3'][target_col][\"val\"] = {\n",
    "#     \"mae\": mae,\n",
    "#     \"rmse\": rmse,\n",
    "#     \"r2\": r2\n",
    "# }\n",
    "\n",
    "\n",
    "# mae, rmse, r2 = evaluate_model(\n",
    "#     model_name, model, test_split, \n",
    "#     feature_cols, target_col,b_sz, device,\n",
    "#     scaler_path=f\"models/notebook/{target_col}_scaler_v3.pkl\", \n",
    "#     seq_len=seq_len\n",
    "# )\n",
    "# result_table['Strategy_3'][target_col][\"test\"] = {\n",
    "#     \"mae\": mae,\n",
    "#     \"rmse\": rmse,\n",
    "#     \"r2\": r2\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36f3b48e-3803-4295-870d-e11323535a0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T05:41:11.915123Z",
     "iopub.status.busy": "2025-01-31T05:41:11.914843Z",
     "iopub.status.idle": "2025-01-31T05:41:11.918791Z",
     "shell.execute_reply": "2025-01-31T05:41:11.918402Z",
     "shell.execute_reply.started": "2025-01-31T05:41:11.915106Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Strategy_3': {'NTotL1': {'train': {'mae': np.float32(17.390928),\n",
       "    'rmse': np.float32(22.136848),\n",
       "    'r2': 0.49526357650756836},\n",
       "   'val': {'mae': np.float32(15.866853),\n",
       "    'rmse': np.float32(21.25926),\n",
       "    'r2': 0.3508443832397461},\n",
       "   'test': {'mae': np.float32(14.44062),\n",
       "    'rmse': np.float32(19.055813),\n",
       "    'r2': 0.39113956689834595}}}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f7823dc-1da2-4a49-b33b-584cf9331fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-31T05:41:11.919360Z",
     "iopub.status.busy": "2025-01-31T05:41:11.919235Z",
     "iopub.status.idle": "2025-01-31T05:41:11.958808Z",
     "shell.execute_reply": "2025-01-31T05:41:11.958379Z",
     "shell.execute_reply.started": "2025-01-31T05:41:11.919347Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strategy</th>\n",
       "      <th>Target</th>\n",
       "      <th>Metric_Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Strategy_3</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">NTotL1</th>\n",
       "      <th>train</th>\n",
       "      <td>17.390928</td>\n",
       "      <td>22.136848</td>\n",
       "      <td>0.495264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>15.866853</td>\n",
       "      <td>21.259260</td>\n",
       "      <td>0.350844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>14.440620</td>\n",
       "      <td>19.055813</td>\n",
       "      <td>0.391140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     mae       rmse        r2\n",
       "Strategy   Target Metric_Type                                \n",
       "Strategy_3 NTotL1 train        17.390928  22.136848  0.495264\n",
       "                  val          15.866853  21.259260  0.350844\n",
       "                  test         14.440620  19.055813  0.391140"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_data = []\n",
    "for strategy, strategy_data in result_table.items():\n",
    "    for target, target_data in strategy_data.items():\n",
    "        for metric_type, metrics in target_data.items():\n",
    "            row = {\n",
    "                \"Strategy\": strategy,\n",
    "                \"Target\": target,\n",
    "                \"Metric_Type\": metric_type\n",
    "            }\n",
    "            row.update(metrics)\n",
    "            flattened_data.append(row)\n",
    "metric_df = pd.DataFrame(flattened_data)\n",
    "metric_df = metric_df.set_index(['Strategy', 'Target', 'Metric_Type'])\n",
    "metric_df = metric_df.sort_values(by=['Strategy', 'Target'])\n",
    "# metric_df.to_csv(\"models/notebook/metrics.csv\", index=False)\n",
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69792c0b-26de-473c-9593-41aa1a0e818b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99ac231d-c32b-44f5-971e-b06136035c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T00:49:30.807385Z",
     "iopub.status.busy": "2025-01-29T00:49:30.807007Z",
     "iopub.status.idle": "2025-01-29T00:49:30.812588Z",
     "shell.execute_reply": "2025-01-29T00:49:30.812223Z",
     "shell.execute_reply.started": "2025-01-29T00:49:30.807367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def evaluate_by_scenario(model, df, feature_cols, target_col, scaler_path, seq_len=None):\n",
    "#     model.eval()\n",
    "#     results = []\n",
    "#     scenario_vars = ['Year', 'PlantingDay', 'Treatment', 'OrgIrrgDep', 'OrgIrrgThresh', 'NFirstApp']\n",
    "#     for scenario_name, group in df.groupby(scenario_vars, observed=True):\n",
    "#         X, y_true = process_data(group, feature_cols, target_col=target_col, seq_len=seq_len)\n",
    "#         X_tensor = torch.FloatTensor(X).to(device)\n",
    "#         with torch.no_grad():\n",
    "#             y_pred = model(X_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "#         y_true = y_true.cpu().numpy().flatten()\n",
    "#         # Denormalize before calculating metrics\n",
    "#         y_pred = inverse_normalize(y_pred, target_col, scaler_path=scaler_path)\n",
    "#         y_true = inverse_normalize(y_true, target_col, scaler_path=scaler_path)\n",
    "        \n",
    "#         metrics = compute_metrics(y_true, y_pred)\n",
    "#         results.append({\n",
    "#             \"Scenario\": \"_\".join([str(s) for s in scenario_name]),\n",
    "#             \"MAE\": mae,\n",
    "#             \"RMSE\": rmse,\n",
    "#             \"R²\": r2\n",
    "#         })\n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     avg_mae = results_df[\"MAE\"].mean()\n",
    "#     avg_rmse = results_df[\"RMSE\"].mean()\n",
    "#     avg_r2 = results_df[\"R²\"].mean()\n",
    "#     return results_df, avg_mae, avg_rmse, avg_r2\n",
    "\n",
    "\n",
    "# def plot_r2_histogram(df, data_type):\n",
    "#     print(\"\")\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     ax = sns.histplot(df[\"R²\"], bins=40)\n",
    "#     plt.title(f\"{data_type.capitalize()}, Distribution of Scenario-Level R² Scores\")\n",
    "#     plt.xlabel(\"R²\")\n",
    "#     plt.ylabel(\"Frequency\")\n",
    "#     for p in ax.patches:\n",
    "#         height = p.get_height()\n",
    "#         if height > 0:  # Only annotate bars with height > 0\n",
    "#             coords = (p.get_x() + p.get_width() / 2.0, height)\n",
    "#             ax.annotate(f'{int(height)}', coords, \n",
    "#                         ha='center', va='bottom', fontsize=8)\n",
    "#     plt.legend([f'Total samples: {len(df)}'], loc='upper left')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a747eb-297e-4bf5-9f0b-a1c1a3a5aaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14baebc-d2ac-41eb-a70c-8317c719c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [512, 10, 5] -> [512, 1]\n",
    "# to:\n",
    "# [512, 150, 5] -> [512, 150]\n",
    "# [X] TCN, LSTM, Trasnformers\n",
    "# R2, MAE, RMSE -> scenario_wise\n",
    "\n",
    "\n",
    "\n",
    "# LR, XgBoost, mlp\n",
    "# [512, 5] -> [512,1]\n",
    "# [1, 5] -> [1, 1]\n",
    "# R2, MAE, RMSE\n",
    "# test_df -> 1 at a time [108,5] -> [108,1] mean(scenario_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "potsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
